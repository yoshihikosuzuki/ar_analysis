{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s$: CARを計算できたPRの文書集合、$s_1 (\\subset s)$: CARが高かったPRの文書集合、$s_2 (:= s - s_1)$: CARが高くなかったPRの文書集合とする。\n",
    "\n",
    "単語$t$に対して、tf-idf値を以下のように定義。\n",
    "\n",
    "$$\n",
    "{\\rm tf}(t, s_1) = \\frac{df(t, s_1)}{|s_1|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm idf}(t) = \\log\\frac{|s|}{df(t,s)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm df}(t, x) = \\left|\\{d\\mid d\\ni t, d\\in x\\}\\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm tf}\\cdot{\\rm idf}(t, s_1) = {\\rm tf}(t, s_1)\\cdot {\\rm idf}(t)\n",
    "$$\n",
    "\n",
    "tf-idfが高い単語から並べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 入力データのファイル名\n",
    "pr_mor_fname = \"../fix_input_data/pressrelease_all_normalized.csv.mor\"   # MeCab で形態素に分解済みのプレスリリース本文\n",
    "car_sig_fname = \"car_exactT_alpha0.1_sig\"\n",
    "car_not_sig_fname = \"car_exactT_alpha0.1_not_sig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import math\n",
    "from scipy import stats\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pr_type</th>\n",
       "      <th>comp_code</th>\n",
       "      <th>car</th>\n",
       "      <th>t_stat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NIKPRLRSP040809_18022003</td>\n",
       "      <td>01: Product</td>\n",
       "      <td>2593</td>\n",
       "      <td>0.079042</td>\n",
       "      <td>1.582459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIKPRLRSP045017_21042003</td>\n",
       "      <td>04: Alliance</td>\n",
       "      <td>7599</td>\n",
       "      <td>0.120927</td>\n",
       "      <td>1.583770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIKPRLRSP050445_10072003</td>\n",
       "      <td>01: Product</td>\n",
       "      <td>7911</td>\n",
       "      <td>0.104491</td>\n",
       "      <td>2.325275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id       pr_type  comp_code       car    t_stat\n",
       "0  NIKPRLRSP040809_18022003   01: Product       2593  0.079042  1.582459\n",
       "1  NIKPRLRSP045017_21042003  04: Alliance       7599  0.120927  1.583770\n",
       "2  NIKPRLRSP050445_10072003   01: Product       7911  0.104491  2.325275"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CAR。tf-idf 計算は、car_sig と car_all = car_sig + car_not_sig の間で行う\n",
    "## CAR が計算できたものだけを取り扱う点、PR ごとに CAR の平均値を取らない点(後述)が前回と異なる。\n",
    "\n",
    "car_sig = pd.read_table(car_sig_fname, sep = '\\t')   # 有意なもの\n",
    "car_not_sig = pd.read_table(car_not_sig_fname, sep = '\\t')   # 有意でないもの\n",
    "car_sig[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_mor = pd.DataFrame()\n",
    "with open(pr_mor_fname, 'r') as f:\n",
    "    for line in f:\n",
    "        article_id, mor = line.strip().split('\\t')\n",
    "        pr_mor = pr_mor.append([[article_id, mor]], ignore_index = True)\n",
    "pr_mor.columns = [\"article_id\", \"pr_mor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-04efeab00c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 全 PR を形態素分解したもの\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpr_mor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_mor_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'article_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pr_mor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpr_mor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:10415)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:10691)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:11437)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas/parser.c:11308)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:27037)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "## 全 PR を形態素分解したもの\n",
    "pr_mor = pd.read_table(pr_mor_fname, sep = '\\t', header = None, names = ['article_id', 'pr_mor'])\n",
    "pr_mor[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df, idf, tf, tf-idf の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 24397 108 24995\n"
     ]
    }
   ],
   "source": [
    "id_set_sig = set(car_sig[\"article_id\"])\n",
    "id_set_not_sig = set(car_not_sig[\"article_id\"])\n",
    "id_set_all = id_set_sig.union(id_set_not_sig)\n",
    "\n",
    "num_sig = len(id_set_sig)   # equal to |s_1|\n",
    "num_all = len(id_set_all)   # equal to |s|\n",
    "print(num_sig, len(id_set_not_sig), len(id_set_sig.intersection(id_set_not_sig)), num_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 有意な CAR を持つ PR 706個のうち、108個が有意でない(他の企業とのマッチングによる) CAR も同時に持っている\n",
    "\n",
    "TODO: これの扱い (現在は有意な PR に含めている; PR ごとに CAR の平均を取ること(前回はこれを採用していた)はしたくない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_sig = pr_mor[[x in id_set_sig for x in list(pr_mor[\"article_id\"])]]   # pr_mor から、article_id が car_sig に含まれるものだけを抽出\n",
    "pr_all = pr_mor[[x in id_set_all for x in list(pr_mor[\"article_id\"])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pr_mor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NIKPRLRSP040809_18022003</td>\n",
       "      <td>発表 日 : 2003年 2月18日 ” 101 ” は おいしい 烏龍茶 の しるし 。 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  article_id  \\\n",
       "10  NIKPRLRSP040809_18022003   \n",
       "\n",
       "                                               pr_mor  \n",
       "10  発表 日 : 2003年 2月18日 ” 101 ” は おいしい 烏龍茶 の しるし 。 ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pr_mor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIKPRLRSP038060_07012003</td>\n",
       "      <td>発表 日 : 2003年 1月7日 中国 で 単音 ・ 和音 着信メロディ 配信 サービス ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NIKPRLRSP040809_18022003</td>\n",
       "      <td>発表 日 : 2003年 2月18日 ” 101 ” は おいしい 烏龍茶 の しるし 。 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  article_id  \\\n",
       "1   NIKPRLRSP038060_07012003   \n",
       "10  NIKPRLRSP040809_18022003   \n",
       "\n",
       "                                               pr_mor  \n",
       "1   発表 日 : 2003年 1月7日 中国 で 単音 ・ 和音 着信メロディ 配信 サービス ...  \n",
       "10  発表 日 : 2003年 2月18日 ” 101 ” は おいしい 烏龍茶 の しるし 。 ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_as_word_set_sig = [set(x.split(' ')) for x in list(pr_sig[\"pr_mor\"])]   # 有意な PR を形態素分解した単語集合からなるリスト\n",
    "pr_as_word_set_all = [set(x.split(' ')) for x in list(pr_all[\"pr_mor\"])]   # 全部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_count = 4#100000   # very sensitive for input data\n",
    "min_count = 4#100   # very sensitive for input data\n",
    "\n",
    "word_count = Counter()\n",
    "for mor in list(pr_all[\"pr_mor\"]):\n",
    "    for w in mor.split(' '):\n",
    "        word_count[w] += 1\n",
    "        \n",
    "## TODO: 単語の純粋な出現回数ではなく、出現したPRの数でフィルターするべき？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = set()   # filtered set of words to be used for tf-idf calculation\n",
    "for w, c in word_count.items():\n",
    "    if min_count <= c and c <= max_count:\n",
    "        words.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'まし',\n",
       " 'れ',\n",
       " 'メロリン',\n",
       " '中',\n",
       " '今回',\n",
       " '単音',\n",
       " '味わい',\n",
       " '品種',\n",
       " '概要',\n",
       " '約',\n",
       " '美麗',\n",
       " '茶',\n",
       " '製法',\n",
       " '鈴',\n",
       " '黄金色'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_pr_num_with_word_in_sig(word):   # これをword set に付いて並列化\n",
    "    count = 0\n",
    "    for mor in pr_as_word_set_sig:\n",
    "        if word in mor:\n",
    "            count += 1\n",
    "    return (word, count)\n",
    "\n",
    "def calc_pr_num_with_word_in_all(word):   # これをword set に付いて並列化\n",
    "    count = 0\n",
    "    for mor in pr_as_word_set_all:\n",
    "        if word in mor:\n",
    "            count += 1\n",
    "    return (word, count)\n",
    "\n",
    "# TODO: 1つにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exe_pool = Pool(2)\n",
    "df_sig = Counter()\n",
    "for ret in exe_pool.imap(calc_pr_num_with_word_in_sig, words):\n",
    "    df_sig[ret[0]] = ret[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'まし': 1,\n",
       "         'れ': 1,\n",
       "         'メロリン': 0,\n",
       "         '中': 1,\n",
       "         '今回': 1,\n",
       "         '単音': 0,\n",
       "         '味わい': 1,\n",
       "         '品種': 1,\n",
       "         '概要': 1,\n",
       "         '約': 0,\n",
       "         '美麗': 0,\n",
       "         '茶': 1,\n",
       "         '製法': 1,\n",
       "         '鈴': 0,\n",
       "         '黄金色': 1})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exe_pool = Pool(2)\n",
    "df_all = Counter()\n",
    "for ret in exe_pool.imap(calc_pr_num_with_word_in_all, words):\n",
    "    df_all[ret[0]] = ret[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'まし': 2,\n",
       "         'れ': 2,\n",
       "         'メロリン': 1,\n",
       "         '中': 1,\n",
       "         '今回': 2,\n",
       "         '単音': 1,\n",
       "         '味わい': 1,\n",
       "         '品種': 1,\n",
       "         '概要': 2,\n",
       "         '約': 1,\n",
       "         '美麗': 1,\n",
       "         '茶': 1,\n",
       "         '製法': 1,\n",
       "         '鈴': 1,\n",
       "         '黄金色': 1})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = {}\n",
    "for w in words:\n",
    "    tfidf[w] = float(df_sig[w]) / num_sig * math.log(float(num_all) / df_all[w], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'まし': 0.019276702417239215,\n",
       " 'れ': 0.019276702417239215,\n",
       " 'メロリン': 0.0,\n",
       " '中': 0.020693133012140066,\n",
       " '今回': 0.019276702417239215,\n",
       " '単音': 0.0,\n",
       " '味わい': 0.020693133012140066,\n",
       " '品種': 0.020693133012140066,\n",
       " '概要': 0.019276702417239215,\n",
       " '約': 0.0,\n",
       " '美麗': 0.0,\n",
       " '茶': 0.020693133012140066,\n",
       " '製法': 0.020693133012140066,\n",
       " '鈴': 0.0,\n",
       " '黄金色': 0.020693133012140066}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24995\t706\n",
      "製法\t0.020693133012140066\t1\t1\n",
      "中\t0.020693133012140066\t1\t1\n",
      "黄金色\t0.020693133012140066\t1\t1\n",
      "茶\t0.020693133012140066\t1\t1\n",
      "品種\t0.020693133012140066\t1\t1\n",
      "味わい\t0.020693133012140066\t1\t1\n",
      "まし\t0.019276702417239215\t2\t1\n",
      "れ\t0.019276702417239215\t2\t1\n",
      "今回\t0.019276702417239215\t2\t1\n",
      "概要\t0.019276702417239215\t2\t1\n",
      "単音\t0.0\t1\t0\n",
      "鈴\t0.0\t1\t0\n",
      "約\t0.0\t1\t0\n",
      "美麗\t0.0\t1\t0\n",
      "メロリン\t0.0\t1\t0\n"
     ]
    }
   ],
   "source": [
    "print(str(num_all) + \"\\t\" + str(num_sig))   # # PRs in all PR set, partial set\n",
    "for data in sorted(tfidf.items(), key=lambda x: x[1], reverse = True):                                             \n",
    "    print('\\t'.join([data[0], str(data[1]), str(df_all[data[0]]), str(df_sig[data[0]])]))   # word, tf-idf, # PR in all PR with the word, # PR in partial set\n",
    "    \n",
    "# TODO: statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
